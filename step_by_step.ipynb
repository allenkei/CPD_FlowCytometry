{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "195fa23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "torch.set_printoptions(precision=5)\n",
    "output_dir = \"C:/Users/lix23/Desktop/CPM/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a4c8a545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(z_dim=10, x_dim=3, y_dim=3, K_dim=3, output_layer=[128, 128], num_samples=100, langevin_K=100, langevin_s=0.1, kappa=0.1, penalties=[0.1, 1, 10, 50, 100, 1000], time_embed_dim=16, epoch=50, decoder_iteration=20, nu_iteration=20, decoder_lr=0.01, decoder_thr=0.001, iter_thr=5, loss_thr=1e-05, use_data='XXX', num_seq=100, num_time=200, data_dir='./data/', f=None)\n",
      "[INFO] cuda:0\n"
     ]
    }
   ],
   "source": [
    "def parse_args():\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  parser.add_argument('--z_dim', default=10, type=int) \n",
    "  parser.add_argument('--x_dim', default=3, type=int) \n",
    "  parser.add_argument('--y_dim', default=3, type=int)\n",
    "  parser.add_argument('--K_dim', default=3, type=int)\n",
    "  parser.add_argument('--output_layer', default=[128,128]) \n",
    "  parser.add_argument('--num_samples', default=100) \n",
    "  parser.add_argument('--langevin_K', default=100)\n",
    "  parser.add_argument('--langevin_s', default=0.1) \n",
    "  parser.add_argument('--kappa', default=0.1)\n",
    "  parser.add_argument('--penalties', default=[0.1,1,10,50,100,1000], type=int)\n",
    "  parser.add_argument('--time_embed_dim', default=16, type=int)\n",
    "  \n",
    "  parser.add_argument('--epoch', default=50)\n",
    "  parser.add_argument('--decoder_iteration', default=20)\n",
    "  parser.add_argument('--nu_iteration', default=20)\n",
    "  parser.add_argument('--decoder_lr', default=0.01)\n",
    "  parser.add_argument('--decoder_thr', default=0.001)\n",
    "  parser.add_argument('--iter_thr', default=5)\n",
    "  parser.add_argument('--loss_thr', default=0.00001)\n",
    "\n",
    "  parser.add_argument('--use_data', default='XXX')\n",
    "  parser.add_argument('--num_seq', default=100)\n",
    "  parser.add_argument('--num_time', default=200)\n",
    "  parser.add_argument('--data_dir', default='./data/')\n",
    "  parser.add_argument('-f', required=False)\n",
    "\n",
    "  args, _ = parser.parse_known_args()\n",
    "  return args\n",
    "\n",
    "\n",
    "args = parse_args()\n",
    "print(args)\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "print('[INFO]', device)\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "# LOSS FUNCTION #   \n",
    "#################\n",
    "\n",
    "# Write a loss function for infer_z() use. This version is idendity matrix case.\n",
    "\n",
    "# def mixture_of_gaussians_loss(y, pi, mean):\n",
    "#     \"\"\"\n",
    "#     y:    (N, D)\n",
    "#     pi:   (N, K)\n",
    "#     mean: (N, K, D)\n",
    "#     \"\"\"\n",
    "#     N, K, D = mean.shape\n",
    "    \n",
    "#     # expand y -> (N, K, D)\n",
    "#     y_expand = y.unsqueeze(1).expand(-1, K, -1)\n",
    "    \n",
    "#     # squared Euclidean distance ||y - mu||^2\n",
    "#     diff_sq = torch.sum((y_expand - mean) ** 2, dim=-1)   # (N, K)\n",
    "    \n",
    "#     # Gaussian pdf with identity covariance\n",
    "#     norm_const = 1.0 / ((2 * torch.pi) ** (D / 2))\n",
    "#     pdf_vals = norm_const * torch.exp(-0.5 * diff_sq)     # (N, K)\n",
    "    \n",
    "#     # mixture: sum_k pi * pdf\n",
    "#     weighted = pi * pdf_vals\n",
    "#     total = torch.sum(weighted, dim=1)                    # (N,)\n",
    "    \n",
    "#     # negative log-likelihood\n",
    "#     nll = -torch.sum(torch.log(total + 1e-10))\n",
    "#     return nll\n",
    "\n",
    "# Finished 2025-08-20 9:48 AM PST\n",
    "\n",
    "# Another Version, diagonal case.\n",
    "\n",
    "\n",
    "def mixture_of_gaussians_loss(y, pi, mean, sigma):\n",
    "    \"\"\"\n",
    "    y:     (N, D)\n",
    "    pi:    (N, K)\n",
    "    mean:  (N, K, D)\n",
    "    sigma: (N, K, D)  \n",
    "    \"\"\"\n",
    "    N, K, D = mean.shape\n",
    "\n",
    "    # expand y to (N, K, D)\n",
    "    y_expand = y.unsqueeze(1).expand(-1, K, -1)\n",
    "\n",
    "    # (y - mu)^2 / sigma\n",
    "    diff = y_expand - mean\n",
    "    mahal = torch.sum((diff**2) / (sigma + 1e-8), dim=2)  # (N, K)\n",
    "\n",
    "    # log det of diagonal cov\n",
    "    log_det = torch.sum(torch.log(sigma + 1e-8), dim=2)  # (N, K)\n",
    "\n",
    "    # log Gaussian pdf\n",
    "    log_prob = -0.5 * (D * np.log(2*np.pi) + log_det + mahal)  # (N, K)\n",
    "\n",
    "    # mixture likelihood\n",
    "    weighted = pi * torch.exp(log_prob)  # (N, K)\n",
    "    total_prob = torch.sum(weighted, dim=1)  # (N,)\n",
    "\n",
    "    # negative log-likelihood\n",
    "    nll = -torch.sum(torch.log(total_prob + 1e-12))\n",
    "    return nll\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c173146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# LOAD SAVED DATA #\n",
    "###################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# x_df = pd.read_csv(\"x_all.csv\")\n",
    "# z_df = pd.read_csv(\"z_all.csv\")\n",
    "# y_df = pd.read_csv(\"y_all.csv\")\n",
    "# meta_df = pd.read_csv(\"data_all.csv\")\n",
    "\n",
    "# x_all = x_df.values   # (20000, 3)\n",
    "# z_all = z_df.values   # (20000, 3)\n",
    "# y_all = y_df.values   # (20000, 3)\n",
    "\n",
    "# # Check dimension match \n",
    "# X_rec = x_all.reshape(args.num_time, args.num_samples, args.x_dim)\n",
    "# # Z_rec = z_all.reshape(args.num_time, args.num_samples, args.z_dim)\n",
    "# Y_rec = y_all.reshape(args.num_time, args.num_samples, args.y_dim)\n",
    "\n",
    "# X_torch = torch.tensor(X_rec, dtype=torch.float32)\n",
    "# # Z_torch = torch.tensor(Z_rec, dtype=torch.float32)\n",
    "# Y_torch = torch.tensor(Y_rec, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Initialize the weights\n",
    "def init_weights(m):\n",
    "  for name, param in m.named_parameters():\n",
    "    nn.init.uniform_(param.data, -0.05, 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0cd58d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# MODEL #\n",
    "#########\n",
    "\n",
    "class CPD(nn.Module):\n",
    "  def __init__(self, args, half):\n",
    "    super(CPD, self).__init__()\n",
    "\n",
    "    self.d = args.z_dim\n",
    "    self.p = args.x_dim\n",
    "    self.K = args.K_dim           # mixture components\n",
    "    self.D = args.y_dim           # output dim = 3\n",
    "    self.T = int(args.num_time/2) if half else args.num_time\n",
    "\n",
    "    self.l1 = nn.Linear(self.d + self.p, args.output_layer[0])\n",
    "    self.l2 = nn.Linear(args.output_layer[0], args.output_layer[1])\n",
    "\n",
    "    self.l3_pi    = nn.Linear(args.output_layer[1], self.K)          # (TB, K)\n",
    "    self.l3_mean  = nn.Linear(args.output_layer[1], self.K * self.D) # (TB, K*3)\n",
    "    self.l3_sigma = nn.Linear(args.output_layer[1], self.K * self.D) # (TB, K*3)\n",
    "    \n",
    "  def forward(self, x, z):\n",
    "    output = torch.cat([x, z], dim=1)\n",
    "    output = self.l1(output).tanh()\n",
    "    output = self.l2(output).tanh()\n",
    "\n",
    "    pi = self.l3_pi(output).softmax(dim=1)                  # (TB, K)\n",
    "    mean = self.l3_mean(output).reshape(-1, self.K, self.D) # (TB, K, D)\n",
    "    sigma = F.softplus(self.l3_sigma(output)).reshape(-1, self.K, self.D) + 1e-6\n",
    "\n",
    "    # covs = []\n",
    "    # for k in range(self.K):\n",
    "    #     # cov_matrix = torch.diag_embed(sigma[:, k, :]**2)    # (TB, D, D)\n",
    "    #     cov_matrix = torch.diag_embed(torch.ones(self.D).to(device))\n",
    "    #     covs.append(cov_matrix)\n",
    "    # covs = torch.stack(covs, dim=1)  # (TB, K, D, D)\n",
    "\n",
    "    return pi, mean, sigma\n",
    "\n",
    "  def infer_z(self, x, z, y, mu_repeat, args):\n",
    "    \"\"\"\n",
    "    x: (T*B, p)\n",
    "    z: (T*B, d)\n",
    "    y: (T*B, D)   # target\n",
    "    mu_repeat: (T*B, d)  # prior mean for z\n",
    "    \"\"\"\n",
    "    for k in range(args.langevin_K):\n",
    "        z = z.detach().clone().requires_grad_(True)\n",
    "\n",
    "        # forward\n",
    "        pi, mean, sigma = self.forward(x, z)\n",
    "\n",
    "        # compute NLL loss\n",
    "        nll = mixture_of_gaussians_loss(y, pi, mean, sigma)\n",
    "\n",
    "        # gradient wrt z\n",
    "        z_grad_nll = torch.autograd.grad(nll, z)[0]\n",
    "\n",
    "        # Langevin noise\n",
    "        noise = torch.randn_like(z).to(z.device)\n",
    "\n",
    "        # update\n",
    "        z = z + args.langevin_s * (-z_grad_nll - (z - mu_repeat)) \\\n",
    "              + torch.sqrt(torch.tensor(2.0 * args.langevin_s)) * noise\n",
    "\n",
    "    return z.detach()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6108f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_one_seq_penalty(args,x_input_train, y_input_train, x_input_test, y_input_test, penalty, half):\n",
    "  m = args.num_samples\n",
    "  kappa = args.kappa\n",
    "  d = args.z_dim\n",
    "  pen_iter = 0\n",
    "  penalty = penalty\n",
    "  print('\\n[INFO] Penalty', penalty)\n",
    "  early_stopping = False\n",
    "  stopping_count = 0 # for ADMM\n",
    "  \n",
    "  T = x_input_train.shape[0] // args.num_samples   \n",
    "  \n",
    "  # create matrix X and vector 1\n",
    "  ones_col = torch.ones(T, 1).to(device)\n",
    "  X = torch.zeros(T, T-1).to(device)\n",
    "  i, j = torch.tril_indices(T, T-1, offset=-1)\n",
    "  X[i, j] = 1 # Group Fused Lasso\n",
    "  \n",
    "  old_loss_train = -float('inf')\n",
    "  loss_train_holder = []\n",
    "  loss_test_holder = []\n",
    "  mu_diff_holder = []\n",
    "  decoder_loss_holder = []\n",
    "  CV_holder = []\n",
    "\n",
    "  if not half:\n",
    "    best_mu = torch.zeros(T,d)\n",
    "    best_loglik = torch.zeros(1)\n",
    "    best_CV = -float('inf') # Coefficient of Variation\n",
    "    best_CV_iter = 0\n",
    "  # initialize mu, nu, w, with dim T by d of zeros\n",
    "  mu = torch.zeros(T, d).to(device)\n",
    "  nu = torch.zeros(T, d).to(device)\n",
    "  w = torch.zeros(T, d).to(device)\n",
    "\n",
    "  mu_old = mu.detach().clone()\n",
    "  nu_old = nu.detach().clone()\n",
    "\n",
    "  # Use y_input directly, no need to repeat\n",
    "\n",
    "  model = CPD(args, half = False).to(device)\n",
    "  model.apply(init_weights)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.decoder_lr)\n",
    "  for learn_iter in range(args.epoch):\n",
    "    print('\\n[INFO] Epoch', learn_iter)\n",
    "    ####################\n",
    "    # GENERATE SAMPLES #\n",
    "    ####################\n",
    "    # create repeated version of mu, from (T by d) to (Tm by d)\n",
    "    mu_repeat = mu.repeat_interleave(m, dim=0) # Tm by d\n",
    "    init_z = torch.randn(T*m, d).to(device) # Tm by d, starts from N(0,1)\n",
    "    print(mu_repeat.shape)\n",
    "    print(init_z.shape)\n",
    "    sampled_z_all = model.infer_z(x_input_train, init_z, y_input_train, mu_repeat, args) # Tm by d\n",
    "\n",
    "    ################\n",
    "    # UPDATE PRIOR # \n",
    "    ################\n",
    "    expected_z = sampled_z_all.clone().reshape(T,m,d) # T by m by d\n",
    "    expected_z = expected_z.mean(dim=1) # T by d\n",
    "    mu = ( expected_z + kappa * (nu-w) ) / ( 1.0 + kappa )\n",
    "    mu = mu.detach().clone()\n",
    "\n",
    "    ##################\n",
    "    # UPDATE DECODER #\n",
    "    ##################\n",
    "    inner_loss = float('inf')\n",
    "    for decoder_iter in range(args.decoder_iteration):\n",
    "        optimizer.zero_grad()\n",
    "        pi, mean, sigma  = model(x_input_train, sampled_z_all) # Tm by n by n \n",
    "        loss_train = mixture_of_gaussians_loss(y_input_train, pi, mean, sigma)/m\n",
    "        loss_train.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "    #########################\n",
    "    # UPDATE BETA AND GAMMA #\n",
    "    #########################\n",
    "  \n",
    "    gamma = nu[0, :].unsqueeze(0) # row vector\n",
    "    beta = torch.diff(nu, dim=0)\n",
    "\n",
    "    for nu_iter in range(args.nu_iteration):\n",
    "        # update beta once (t range from 1 to 99, NOT 1 to 100)\n",
    "        for t in range(T-1): \n",
    "          beta_without_t = beta.detach().clone()\n",
    "          X_without_t = X.detach().clone()\n",
    "          beta_without_t[t,:] = torch.zeros(d) # make this row zeros\n",
    "          X_without_t[:,t] = torch.zeros(T) # make this column zeros\n",
    "          bt = kappa * torch.mm( X[:,t].unsqueeze(0), mu + w - torch.mm(ones_col, gamma) - torch.mm(X_without_t, beta_without_t) )\n",
    "          bt_norm = torch.norm(bt, p=2)\n",
    "\n",
    "          # UPDATE: soft-thresholding\n",
    "          if bt_norm < penalty:\n",
    "              beta[t,:] = torch.zeros(d)\n",
    "          else:\n",
    "              beta[t,:] = 1 / (kappa * torch.norm(X[:,t], p=2)**2) * (1 - penalty/bt_norm) * bt\n",
    "          beta = beta.detach().clone()\n",
    "\n",
    "        # update gamma\n",
    "        gamma = torch.mean(mu + w - torch.mm(X, beta), dim=0).unsqueeze(0).detach().clone()\n",
    "\n",
    "    # recollect nu\n",
    "    nu = torch.mm(ones_col, gamma) + torch.mm(X, beta)\n",
    "    nu = nu.detach().clone()\n",
    "\n",
    "    ############\n",
    "    # UPDATE W # \n",
    "    ############\n",
    "\n",
    "    w = mu - nu + w\n",
    "    w = w.detach().clone()\n",
    "\n",
    "    ############\n",
    "    # RESIDUAL # \n",
    "    ############\n",
    "\n",
    "    primal_residual = torch.sqrt(torch.mean(torch.square(mu - nu)))\n",
    "    dual_residual = torch.sqrt(torch.mean(torch.square(nu - nu_old)))\n",
    "\n",
    "    # if primal_residual > 10.0 * dual_residual:\n",
    "    #     kappa *= 2.0\n",
    "    #     w *= 0.5\n",
    "    #     print('\\n[INFO] kappa increased to', kappa)\n",
    "    # elif dual_residual > 10.0 * primal_residual:\n",
    "    #     kappa *= 0.5\n",
    "    #     w *= 2.0\n",
    "    #     print('\\n[INFO] kappa decreased to', kappa)\n",
    "    \n",
    "    # with torch.no_grad():      \n",
    "      # # Early Stopping - llh\n",
    "      # if half:\n",
    "      #   loss_train = mixture_of_gaussians_loss(y_input_train, pi, mean, sigma)/m # USE TRAIN\n",
    "      #   loss_train_holder.append(loss_train.detach().cpu().numpy().item())\n",
    "      #   mu_repeat = mu.repeat_interleave(m, dim=0) # Tm by d\n",
    "      #   init_z = torch.randn(T*m, d).to(device) # Tm by d, starts from N(0,1)\n",
    "      #   sampled_z_all_test = model.infer_z(x_input_test, init_z, y_input_test, mu_repeat, args) # Tm by d\n",
    "      #   pi_test, mean_test, sigma_test  = model(x_input_test, sampled_z_all_test) # Tm by n by n \n",
    "      #   loss_test = mixture_of_gaussians_loss(y_input_test, pi_test, mean_test, sigma_test)/m\n",
    "      #   loss_test_holder.append(loss_test.detach().cpu().numpy().item())\n",
    "      # else:\n",
    "      #   loss_train = mixture_of_gaussians_loss(y_input_train, pi, mean, sigma)/m # USE TRAIN\n",
    "      #   loss_train_holder.append(loss_train.detach().cpu().numpy().item())\n",
    "      # Early Stopping - llh\n",
    "    if half:\n",
    "        # training loss\n",
    "        loss_train = mixture_of_gaussians_loss(y_input_train, pi, mean, sigma) / m\n",
    "        loss_train_holder.append(loss_train.detach().cpu().item())\n",
    "\n",
    "        # freeze model parameters (no grad for θ)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # still allow z to require grad\n",
    "        mu_repeat = mu.repeat_interleave(m, dim=0)  # (T*m, d)\n",
    "        init_z = torch.randn(T*m, d, device=device, requires_grad=True)\n",
    "\n",
    "        # inference step\n",
    "        sampled_z_all_test = model.infer_z(x_input_test, init_z, y_input_test, mu_repeat, args)\n",
    "        pi_test, mean_test, sigma_test = model(x_input_test, sampled_z_all_test)\n",
    "\n",
    "        # test loss\n",
    "        loss_test = mixture_of_gaussians_loss(y_input_test, pi_test, mean_test, sigma_test) / m\n",
    "        loss_test_holder.append(loss_test.detach().cpu().item())\n",
    "\n",
    "        # restore model parameters to be trainable for next epoch\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        # only training loss\n",
    "        loss_train = mixture_of_gaussians_loss(y_input_train, pi, mean, sigma) / m\n",
    "        loss_train_holder.append(loss_train.detach().cpu().item())\n",
    "\n",
    "    loss_relative_diff = torch.abs((loss_train - old_loss_train) / old_loss_train)\n",
    "    old_loss_train = loss_train.detach().clone()\n",
    "    # mu diff criteria\n",
    "    mu_relative_diff = torch.norm(mu-mu_old, p='fro')\n",
    "    mu_diff_holder.append(mu_relative_diff.detach().cpu().numpy().item())\n",
    "    \n",
    "    mu_old = mu.detach().clone()\n",
    "    nu_old = nu.detach().clone()\n",
    "    #####################\n",
    "    # STOPPING CRITERIA #\n",
    "    #####################\n",
    "\n",
    "    if loss_relative_diff < args.loss_thr:\n",
    "      stopping_count += 1\n",
    "    else:\n",
    "      stopping_count = 0\n",
    "\n",
    "    if stopping_count >= args.iter_thr:\n",
    "      print('\\n[INFO] early stopping')\n",
    "      early_stopping = True        \n",
    "      \n",
    "      \n",
    "    if (learn_iter+1) % 10 == 0:\n",
    "      # print(learn_iter)\n",
    "      with torch.no_grad():\n",
    "        # second row - first row\n",
    "        delta_mu = torch.norm(torch.diff(mu, dim=0), p=2, dim=1)\n",
    "        delta_mu = delta_mu.cpu().detach().numpy() # numpy for plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "      # subplot 1: delta_mu\n",
    "        axes[0].plot(delta_mu)\n",
    "        if not half:\n",
    "          axes[0].axvline(x=args.num_time/2-1, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "        axes[0].set_title(\"Δμ (row diff norm)\")\n",
    "\n",
    "        # # subplot 2: log-likelihood\n",
    "        # axes[0,1].plot(loglik_train_holder[1:])\n",
    "        # axes[0,1].set_title(\"Training log-likelihood\")\n",
    "\n",
    "        # subplot 3: mu difference holder\n",
    "        axes[1].plot(mu_diff_holder[1:])\n",
    "        axes[1].set_title(\"μ diff holder\")\n",
    "\n",
    "        # # subplot 4: CV holder\n",
    "        # axes[1,1].plot(CV_holder)\n",
    "        # axes[1,1].set_title(\"CV holder\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # print('\\nlearning iter (seq={}, [penalty={}], data={}) ='.format(seq_iter,penalty,label), learn_iter+1, 'of', args.epoch)\n",
    "        print('\\ttrain_loss =', loss_train.item())\n",
    "        print('\\tprimal residual =', primal_residual)\n",
    "        print('\\tdual residual =', dual_residual)\n",
    "        print('\\t\\tloss relative difference =', loss_relative_diff)\n",
    "        print('\\t\\tmu relative difference =', mu_relative_diff)\n",
    "    ###############\n",
    "    # SAVE RESULT #\n",
    "    ###############\n",
    "    # at the last iteration or early_stopping\n",
    "    if (learn_iter+1) == args.epoch or early_stopping:\n",
    "\n",
    "      with torch.no_grad():\n",
    "        if half: # Use this to do cv\n",
    "          return loss_test.detach().cpu().numpy(), penalty\n",
    "        else:\n",
    "          # USE THE BEST MU\n",
    "          return loss_train.detach().cpu().numpy(), mu.detach().cpu().numpy()\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1cb620e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = pd.read_csv(\"./sim_data/x_all_1comp.csv\").iloc[:, 2:].values  # (T*N, dx)\n",
    "z_all = pd.read_csv(\"./sim_data/z_all_1comp.csv\").iloc[:, 2:].values   # (T*N, dz)\n",
    "y_all = pd.read_csv(\"./sim_data/y_all_1comp.csv\").iloc[:, 2:].values   # (T*N, dy)\n",
    "\n",
    "x_all = pd.read_csv(\"./sim_data/x_all.csv\").iloc[:, 2:].values   # (T*N, dx)\n",
    "z_all = pd.read_csv(\"./sim_data/z_all.csv\").iloc[:, 2:].values   # (T*N, dz)\n",
    "y_all = pd.read_csv(\"./sim_data/y_all.csv\").iloc[:, 2:].values   # (T*N, dy)\n",
    "\n",
    "\n",
    "x_input = torch.tensor(x_all, dtype=torch.float32).to(device)\n",
    "z_input = torch.tensor(z_all, dtype=torch.float32).to(device)\n",
    "y_input = torch.tensor(y_all, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important : k = 1, dz = 5. kappa = 0.5, penalty = 10000, epoch = 30  is good\n",
    "# Important : k = 3, dz = 10, epoch = 50, decoder_lr = 0.01, penalty = 10000, is good\n",
    "args.epoch = 30\n",
    "args.z_dim = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c970ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "x_input = torch.tensor(x_all, dtype=torch.float32).reshape(args.num_time, args.num_samples, args.x_dim).to(device)\n",
    "y_input = torch.tensor(y_all, dtype=torch.float32).reshape(args.num_time, args.num_samples, args.y_dim).to(device)\n",
    "\n",
    "\n",
    "odd_idx = range(1, args.num_time, 2)\n",
    "even_idx = range(0, args.num_time, 2)\n",
    "x_input_train = x_input[odd_idx, :, :]\n",
    "x_input_test  = x_input[even_idx, :, :]\n",
    "y_input_train = y_input[odd_idx, :, :]\n",
    "y_input_test  = y_input[even_idx, :, :]\n",
    "\n",
    "x_input_train = x_input_train.reshape(-1, args.x_dim)   # (T/2 * N, x_dim)\n",
    "x_input_test  = x_input_test.reshape(-1, args.x_dim)\n",
    "y_input_train = y_input_train.reshape(-1, args.y_dim)\n",
    "y_input_test  = y_input_test.reshape(-1, args.y_dim)\n",
    "\n",
    "\n",
    "# res_half = learn_one_seq_penalty(args, x_input_train, y_input_train,x_input_test, y_input_test, penalty=1, half=True)\n",
    "# res_half\n",
    "\n",
    "results = []\n",
    "\n",
    "for penalty in args.penalties: \n",
    "    a, b = learn_one_seq_penalty(\n",
    "        args,\n",
    "        x_input_train, y_input_train,\n",
    "        x_input_test, y_input_test,\n",
    "        penalty=penalty,\n",
    "        half=True\n",
    "    )\n",
    "    results.append([a, b])\n",
    "\n",
    "results = np.array(results)   # shape = (len(args.penalties), 2)\n",
    "print(results.shape)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "322b8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = pd.read_csv(\"./sim_data/x_all_1comp.csv\").iloc[:, 2:].values  # (T*N, dx)\n",
    "z_all = pd.read_csv(\"./sim_data/z_all_1comp.csv\").iloc[:, 2:].values   # (T*N, dz)\n",
    "y_all = pd.read_csv(\"./sim_data/y_all_1comp.csv\").iloc[:, 2:].values   # (T*N, dy)\n",
    "\n",
    "x_all = pd.read_csv(\"./sim_data/x_all.csv\").iloc[:, 2:].values   # (T*N, dx)\n",
    "z_all = pd.read_csv(\"./sim_data/z_all.csv\").iloc[:, 2:].values   # (T*N, dz)\n",
    "y_all = pd.read_csv(\"./sim_data/y_all.csv\").iloc[:, 2:].values   # (T*N, dy)\n",
    "\n",
    "\n",
    "x_input = torch.tensor(x_all, dtype=torch.float32).to(device)\n",
    "z_input = torch.tensor(z_all, dtype=torch.float32).to(device)\n",
    "y_input = torch.tensor(y_all, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6bafeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test passed.\n",
    "best_idx = np.argmin(results[:, 0])\n",
    "\n",
    "res_full = learn_one_seq_penalty(\n",
    "    args,\n",
    "    x_input, y_input,\n",
    "    x_input, y_input,\n",
    "    penalty=args.penalties[best_idx],\n",
    "    half=False\n",
    ")\n",
    "res_full"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CPD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
