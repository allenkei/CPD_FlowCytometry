{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "195fa23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "torch.set_printoptions(precision=5)\n",
    "output_dir = \"C:/Users/lix23/Desktop/CPM/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4c8a545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(z_dim=3, x_dim=3, y_dim=3, K_dim=3, output_layer=[32, 64], num_samples=100, langevin_K=100, langevin_s=0.1, kappa=10, penalties=[10, 20, 50, 100], time_embed_dim=16, epoch=50, decoder_iteration=20, nu_iteration=20, decoder_lr=0.1, decoder_thr=0.001, iter_thr=5, loglik_thr=1e-05, use_data='XXX', num_seq=100, num_time=200, data_dir='./data/', f=None)\n",
      "[INFO] cuda:0\n"
     ]
    }
   ],
   "source": [
    "def parse_args():\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  parser.add_argument('--z_dim', default=3, type=int) \n",
    "  parser.add_argument('--x_dim', default=3, type=int) \n",
    "  parser.add_argument('--y_dim', default=3, type=int)\n",
    "  parser.add_argument('--K_dim', default=3, type=int)\n",
    "  parser.add_argument('--output_layer', default=[32,64]) \n",
    "  parser.add_argument('--num_samples', default=100) \n",
    "  parser.add_argument('--langevin_K', default=100)\n",
    "  parser.add_argument('--langevin_s', default=0.1) \n",
    "  parser.add_argument('--kappa', default=10)\n",
    "  parser.add_argument('--penalties', default=[10,20,50,100], type=int)\n",
    "  parser.add_argument('--time_embed_dim', default=16, type=int)\n",
    "  \n",
    "  parser.add_argument('--epoch', default=50)\n",
    "  parser.add_argument('--decoder_iteration', default=20)\n",
    "  parser.add_argument('--nu_iteration', default=20)\n",
    "  parser.add_argument('--decoder_lr', default=0.1)\n",
    "  parser.add_argument('--decoder_thr', default=0.001)\n",
    "  parser.add_argument('--iter_thr', default=5)\n",
    "  parser.add_argument('--loglik_thr', default=0.00001)\n",
    "\n",
    "  parser.add_argument('--use_data', default='XXX')\n",
    "  parser.add_argument('--num_seq', default=100)\n",
    "  parser.add_argument('--num_time', default=200)\n",
    "  parser.add_argument('--data_dir', default='./data/')\n",
    "  parser.add_argument('-f', required=False)\n",
    "\n",
    "  args, _ = parser.parse_known_args()\n",
    "  return args\n",
    "\n",
    "\n",
    "args = parse_args()\n",
    "print(args)\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "print('[INFO]', device)\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "# LOSS FUNCTION #   \n",
    "#################\n",
    "\n",
    "# Write a loss function for infer_z() use. This version is idendity matrix case.\n",
    "\n",
    "# def mixture_of_gaussians_loss(y, pi, mean):\n",
    "#     \"\"\"\n",
    "#     y:    (N, D)\n",
    "#     pi:   (N, K)\n",
    "#     mean: (N, K, D)\n",
    "#     \"\"\"\n",
    "#     N, K, D = mean.shape\n",
    "    \n",
    "#     # expand y -> (N, K, D)\n",
    "#     y_expand = y.unsqueeze(1).expand(-1, K, -1)\n",
    "    \n",
    "#     # squared Euclidean distance ||y - mu||^2\n",
    "#     diff_sq = torch.sum((y_expand - mean) ** 2, dim=-1)   # (N, K)\n",
    "    \n",
    "#     # Gaussian pdf with identity covariance\n",
    "#     norm_const = 1.0 / ((2 * torch.pi) ** (D / 2))\n",
    "#     pdf_vals = norm_const * torch.exp(-0.5 * diff_sq)     # (N, K)\n",
    "    \n",
    "#     # mixture: sum_k pi * pdf\n",
    "#     weighted = pi * pdf_vals\n",
    "#     total = torch.sum(weighted, dim=1)                    # (N,)\n",
    "    \n",
    "#     # negative log-likelihood\n",
    "#     nll = -torch.sum(torch.log(total + 1e-10))\n",
    "#     return nll\n",
    "\n",
    "# Finished 2025-08-20 9:48 AM PST\n",
    "\n",
    "# Another Version, diagonal case.\n",
    "\n",
    "\n",
    "def mixture_of_gaussians_loss(y, pi, mean, sigma):\n",
    "    \"\"\"\n",
    "    y:     (N, D)\n",
    "    pi:    (N, K)\n",
    "    mean:  (N, K, D)\n",
    "    sigma: (N, K, D)  \n",
    "    \"\"\"\n",
    "    N, K, D = mean.shape\n",
    "\n",
    "    # expand y to (N, K, D)\n",
    "    y_expand = y.unsqueeze(1).expand(-1, K, -1)\n",
    "\n",
    "    # (y - mu)^2 / sigma\n",
    "    diff = y_expand - mean\n",
    "    mahal = torch.sum((diff**2) / (sigma + 1e-8), dim=2)  # (N, K)\n",
    "\n",
    "    # log det of diagonal cov\n",
    "    log_det = torch.sum(torch.log(sigma + 1e-8), dim=2)  # (N, K)\n",
    "\n",
    "    # log Gaussian pdf\n",
    "    log_prob = -0.5 * (D * np.log(2*np.pi) + log_det + mahal)  # (N, K)\n",
    "\n",
    "    # mixture likelihood\n",
    "    weighted = pi * torch.exp(log_prob)  # (N, K)\n",
    "    total_prob = torch.sum(weighted, dim=1)  # (N,)\n",
    "\n",
    "    # negative log-likelihood\n",
    "    nll = -torch.sum(torch.log(total_prob + 1e-12))\n",
    "    return nll\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c173146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# LOAD SAVED DATA #\n",
    "###################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "x_df = pd.read_csv(\"x_all.csv\")\n",
    "z_df = pd.read_csv(\"z_all.csv\")\n",
    "y_df = pd.read_csv(\"y_all.csv\")\n",
    "meta_df = pd.read_csv(\"data_all.csv\")\n",
    "\n",
    "x_all = x_df.values   # (20000, 3)\n",
    "z_all = z_df.values   # (20000, 3)\n",
    "y_all = y_df.values   # (20000, 3)\n",
    "\n",
    "# Check dimension match \n",
    "X_rec = x_all.reshape(args.num_time, args.num_samples, args.x_dim)\n",
    "# Z_rec = z_all.reshape(args.num_time, args.num_samples, args.z_dim)\n",
    "Y_rec = y_all.reshape(args.num_time, args.num_samples, args.y_dim)\n",
    "\n",
    "X_torch = torch.tensor(X_rec, dtype=torch.float32)\n",
    "# Z_torch = torch.tensor(Z_rec, dtype=torch.float32)\n",
    "Y_torch = torch.tensor(Y_rec, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Initialize the weights\n",
    "def init_weights(m):\n",
    "  for name, param in m.named_parameters():\n",
    "    nn.init.uniform_(param.data, -0.05, 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0cd58d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# MODEL #\n",
    "#########\n",
    "\n",
    "class CPD(nn.Module):\n",
    "  def __init__(self, args, half):\n",
    "    super(CPD, self).__init__()\n",
    "\n",
    "    self.d = args.z_dim\n",
    "    self.p = args.x_dim\n",
    "    self.K = args.K_dim           # mixture components\n",
    "    self.D = args.y_dim           # output dim = 3\n",
    "    self.T = int(args.num_time/2) if half else args.num_time\n",
    "\n",
    "    self.l1 = nn.Linear(self.d + self.p, args.output_layer[0])\n",
    "    self.l2 = nn.Linear(args.output_layer[0], args.output_layer[1])\n",
    "\n",
    "    self.l3_pi    = nn.Linear(args.output_layer[1], self.K)          # (TB, K)\n",
    "    self.l3_mean  = nn.Linear(args.output_layer[1], self.K * self.D) # (TB, K*3)\n",
    "    self.l3_sigma = nn.Linear(args.output_layer[1], self.K * self.D) # (TB, K*3)\n",
    "    \n",
    "  def forward(self, x, z):\n",
    "    output = torch.cat([x, z], dim=1)\n",
    "    output = self.l1(output).tanh()\n",
    "    output = self.l2(output).tanh()\n",
    "\n",
    "    pi = self.l3_pi(output).softmax(dim=1)                  # (TB, K)\n",
    "    mean = self.l3_mean(output).reshape(-1, self.K, self.D) # (TB, K, D)\n",
    "    sigma = F.softplus(self.l3_sigma(output)).reshape(-1, self.K, self.D) + 1e-6\n",
    "\n",
    "    # covs = []\n",
    "    # for k in range(self.K):\n",
    "    #     # cov_matrix = torch.diag_embed(sigma[:, k, :]**2)    # (TB, D, D)\n",
    "    #     cov_matrix = torch.diag_embed(torch.ones(self.D).to(device))\n",
    "    #     covs.append(cov_matrix)\n",
    "    # covs = torch.stack(covs, dim=1)  # (TB, K, D, D)\n",
    "\n",
    "    return pi, mean, sigma\n",
    "\n",
    "  def infer_z(self, x, z, y, mu_repeat, args):\n",
    "    \"\"\"\n",
    "    x: (T*B, p)\n",
    "    z: (T*B, d)\n",
    "    y: (T*B, D)   # target\n",
    "    mu_repeat: (T*B, d)  # prior mean for z\n",
    "    \"\"\"\n",
    "    for k in range(args.langevin_K):\n",
    "        z = z.detach().clone().requires_grad_(True)\n",
    "\n",
    "        # forward\n",
    "        pi, mean, sigma = self.forward(x, z)\n",
    "\n",
    "        # compute NLL loss\n",
    "        nll = mixture_of_gaussians_loss(y, pi, mean, sigma)\n",
    "\n",
    "        # gradient wrt z\n",
    "        z_grad_nll = torch.autograd.grad(nll, z)[0]\n",
    "\n",
    "        # Langevin noise\n",
    "        noise = torch.randn_like(z).to(z.device)\n",
    "\n",
    "        # update\n",
    "        z = z + args.langevin_s * (-z_grad_nll - (z - mu_repeat)) \\\n",
    "              + torch.sqrt(torch.tensor(2.0 * args.langevin_s)) * noise\n",
    "\n",
    "    return z.detach()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1cb620e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = pd.read_csv(\"x_all.csv\").values   # (T*N, dx)\n",
    "z_all = pd.read_csv(\"z_all.csv\").values   # (T*N, dz)\n",
    "y_all = pd.read_csv(\"y_all.csv\").values   # (T*N, dy)\n",
    "\n",
    "\n",
    "x_input = torch.tensor(x_all, dtype=torch.float32).to(device)\n",
    "z_input = torch.tensor(z_all, dtype=torch.float32).to(device)\n",
    "y_input = torch.tensor(y_all, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6108f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_one_seq_penalty(args,x_input, y_input, penalty):\n",
    "    m = args.num_samples\n",
    "    kappa = args.kappa\n",
    "    d = args.z_dim\n",
    "    pen_iter = 0\n",
    "    penalty = penalty\n",
    "    early_stopping = False\n",
    "    stopping_count = 0 # for ADMM\n",
    "    # create matrix X and vector 1\n",
    "    T = args.num_time\n",
    "    ones_col = torch.ones(T, 1).to(device)\n",
    "    X = torch.zeros(T, T-1).to(device)\n",
    "    i, j = torch.tril_indices(T, T-1, offset=-1)\n",
    "    X[i, j] = 1 # Group Fused Lasso\n",
    "    old_loglik = -float('inf')\n",
    "    loglik_train_holder = []\n",
    "    loglik_test_holder = []\n",
    "    mu_diff_holder = []\n",
    "    decoder_loss_holder = []\n",
    "    CV_holder = []\n",
    "    best_mu = torch.zeros(T,d)\n",
    "    best_loglik = torch.zeros(1)\n",
    "    best_CV = -float('inf') # Coefficient of Variation\n",
    "    best_CV_iter = 0\n",
    "\n",
    "    # initialize mu, nu, w, with dim T by d of zeros\n",
    "    mu = torch.zeros(T, d).to(device)\n",
    "    nu = torch.zeros(T, d).to(device)\n",
    "    w = torch.zeros(T, d).to(device)\n",
    "\n",
    "    mu_old = mu.detach().clone()\n",
    "    nu_old = nu.detach().clone()\n",
    "\n",
    "    print(y_input.shape)\n",
    "    # Use y_input directly, no need to repeat\n",
    "\n",
    "    model = CPD(args, half = False).to(device)\n",
    "    model.apply(init_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.decoder_lr)\n",
    "    for learn_iter in range(args.epoch):\n",
    "      print('\\n[INFO] Epoch', learn_iter)\n",
    "      ####################\n",
    "      # GENERATE SAMPLES #\n",
    "      ####################\n",
    "      # create repeated version of mu, from (T by d) to (Tm by d)\n",
    "      mu_repeat = mu.repeat_interleave(m, dim=0) # Tm by d\n",
    "      init_z = torch.randn(T*m, d).to(device) # Tm by d, starts from N(0,1)\n",
    "      sampled_z_all = model.infer_z(x_input, init_z, y_input, mu_repeat, args) # Tm by d\n",
    "\n",
    "      ################\n",
    "      # UPDATE PRIOR # \n",
    "      ################\n",
    "      expected_z = sampled_z_all.clone().reshape(T,m,d) # T by m by d\n",
    "      expected_z = expected_z.mean(dim=1) # T by d\n",
    "      mu = ( expected_z + kappa * (nu-w) ) / ( 1.0 + kappa )\n",
    "      mu = mu.detach().clone()\n",
    "\n",
    "      ##################\n",
    "      # UPDATE DECODER #\n",
    "      ##################\n",
    "      inner_loss = float('inf')\n",
    "      for decoder_iter in range(args.decoder_iteration):\n",
    "          optimizer.zero_grad()\n",
    "          pi, mean, sigma  = model(x_input, sampled_z_all) # Tm by n by n \n",
    "          loss = mixture_of_gaussians_loss(y_input, pi, mean, sigma)/m\n",
    "          loss.backward()\n",
    "          nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "          optimizer.step()\n",
    "\n",
    "      #########################\n",
    "      # UPDATE BETA AND GAMMA #\n",
    "      #########################\n",
    "    \n",
    "      gamma = nu[0, :].unsqueeze(0) # row vector\n",
    "      beta = torch.diff(nu, dim=0)\n",
    "\n",
    "      for nu_iter in range(args.nu_iteration):\n",
    "          # update beta once (t range from 1 to 99, NOT 1 to 100)\n",
    "          for t in range(T-1): \n",
    "            beta_without_t = beta.detach().clone()\n",
    "            X_without_t = X.detach().clone()\n",
    "            beta_without_t[t,:] = torch.zeros(d) # make this row zeros\n",
    "            X_without_t[:,t] = torch.zeros(T) # make this column zeros\n",
    "            bt = kappa * torch.mm( X[:,t].unsqueeze(0), mu + w - torch.mm(ones_col, gamma) - torch.mm(X_without_t, beta_without_t) )\n",
    "            bt_norm = torch.norm(bt, p=2)\n",
    "\n",
    "            # UPDATE: soft-thresholding\n",
    "            if bt_norm < penalty:\n",
    "                beta[t,:] = torch.zeros(d)\n",
    "            else:\n",
    "                beta[t,:] = 1 / (kappa * torch.norm(X[:,t], p=2)**2) * (1 - penalty/bt_norm) * bt\n",
    "            beta = beta.detach().clone()\n",
    "\n",
    "          # update gamma\n",
    "          gamma = torch.mean(mu + w - torch.mm(X, beta), dim=0).unsqueeze(0).detach().clone()\n",
    "\n",
    "      # recollect nu\n",
    "      nu = torch.mm(ones_col, gamma) + torch.mm(X, beta)\n",
    "      nu = nu.detach().clone()\n",
    "\n",
    "      ############\n",
    "      # UPDATE W # \n",
    "      ############\n",
    "\n",
    "      w = mu - nu + w\n",
    "      w = w.detach().clone()\n",
    "\n",
    "      ############\n",
    "      # RESIDUAL # \n",
    "      ############\n",
    "\n",
    "      primal_residual = torch.sqrt(torch.mean(torch.square(mu - nu)))\n",
    "      dual_residual = torch.sqrt(torch.mean(torch.square(nu - nu_old)))\n",
    "\n",
    "      # if primal_residual > 10.0 * dual_residual:\n",
    "      #     kappa *= 2.0\n",
    "      #     w *= 0.5\n",
    "      #     print('\\n[INFO] kappa increased to', kappa)\n",
    "      # elif dual_residual > 10.0 * primal_residual:\n",
    "      #     kappa *= 0.5\n",
    "      #     w *= 2.0\n",
    "      #     print('\\n[INFO] kappa decreased to', kappa)\n",
    "          \n",
    "      with torch.no_grad():\n",
    "        # loglik = model.cal_loglik(mu, adj_train_repeat)\n",
    "        # loglik_train_holder.append(loglik.detach().cpu().numpy().item())\n",
    "        \n",
    "      #   # criteria 1\n",
    "      #   loglik_relative_diff = torch.abs((loglik - old_loglik) / old_loglik)\n",
    "      #   old_loglik = loglik.detach().clone()\n",
    "        # criteria 2\n",
    "        mu_relative_diff = torch.norm(mu-mu_old, p='fro')\n",
    "        mu_diff_holder.append(mu_relative_diff.detach().cpu().numpy().item())\n",
    "        \n",
    "        mu_old = mu.detach().clone()\n",
    "        nu_old = nu.detach().clone()\n",
    "        \n",
    "        \n",
    "        \n",
    "      if (learn_iter+1) % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "          # second row - first row\n",
    "          delta_mu = torch.norm(torch.diff(mu, dim=0), p=2, dim=1)\n",
    "          delta_mu = delta_mu.cpu().detach().numpy() # numpy for plot\n",
    "          if (learn_iter+1) % 10 == 0:\n",
    "            fig, axes = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "          # subplot 1: delta_mu\n",
    "            axes.plot(delta_mu)\n",
    "            axes.set_title(\"Δμ (row diff norm)\")\n",
    "\n",
    "          # # subplot 2: log-likelihood\n",
    "          # axes[0,1].plot(loglik_train_holder[1:])\n",
    "          # axes[0,1].set_title(\"Training log-likelihood\")\n",
    "\n",
    "          # # subplot 3: mu difference holder\n",
    "          # axes[1].plot(mu_diff_holder[1:])\n",
    "          # axes[1].set_title(\"μ diff holder\")\n",
    "\n",
    "          # # subplot 4: CV holder\n",
    "          # axes[1,1].plot(CV_holder)\n",
    "          # axes[1,1].set_title(\"CV holder\")\n",
    "\n",
    "          # plt.tight_layout()\n",
    "          # plt.show()\n",
    "\n",
    "          # print('\\nlearning iter (seq={}, [penalty={}], data={}) ='.format(seq_iter,penalty,label), learn_iter+1, 'of', args.epoch)\n",
    "          print('\\tlog likelihood =', loss.item())\n",
    "          print('\\tprimal residual =', primal_residual)\n",
    "          print('\\tdual residual =', dual_residual)\n",
    "          # print('\\t\\tlog likelihood relative difference =', loglik_relative_diff)\n",
    "          # print('\\t\\tmu relative difference =', mu_relative_diff)       \n",
    "    return(mu)  \n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9dc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 3])\n",
      "\n",
      "[INFO] Epoch 0\n",
      "\tlog likelihood = 3238.771484375\n",
      "\tprimal residual = tensor(0.01000, device='cuda:0')\n",
      "\tdual residual = tensor(0.00054, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 1\n",
      "\tlog likelihood = 3198.4287109375\n",
      "\tprimal residual = tensor(0.01291, device='cuda:0')\n",
      "\tdual residual = tensor(0.00206, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 2\n",
      "\tlog likelihood = 3184.3115234375\n",
      "\tprimal residual = tensor(0.01173, device='cuda:0')\n",
      "\tdual residual = tensor(0.00375, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 3\n",
      "\tlog likelihood = 3180.123779296875\n",
      "\tprimal residual = tensor(0.01233, device='cuda:0')\n",
      "\tdual residual = tensor(0.00190, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 4\n",
      "\tlog likelihood = 3170.5615234375\n",
      "\tprimal residual = tensor(0.01233, device='cuda:0')\n",
      "\tdual residual = tensor(0.00123, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 5\n",
      "\tlog likelihood = 3167.7744140625\n",
      "\tprimal residual = tensor(0.01187, device='cuda:0')\n",
      "\tdual residual = tensor(0.00083, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 6\n",
      "\tlog likelihood = 3172.175537109375\n",
      "\tprimal residual = tensor(0.01191, device='cuda:0')\n",
      "\tdual residual = tensor(0.00032, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 7\n",
      "\tlog likelihood = 3168.47705078125\n",
      "\tprimal residual = tensor(0.01186, device='cuda:0')\n",
      "\tdual residual = tensor(0.00075, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 8\n",
      "\tlog likelihood = 3167.878662109375\n",
      "\tprimal residual = tensor(0.01210, device='cuda:0')\n",
      "\tdual residual = tensor(0.00075, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 9\n",
      "\tlog likelihood = 3167.27294921875\n",
      "\tprimal residual = tensor(0.01258, device='cuda:0')\n",
      "\tdual residual = tensor(0.00118, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 10\n",
      "\tlog likelihood = 3166.941162109375\n",
      "\tprimal residual = tensor(0.01183, device='cuda:0')\n",
      "\tdual residual = tensor(0.00086, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 11\n",
      "\tlog likelihood = 3166.2724609375\n",
      "\tprimal residual = tensor(0.01180, device='cuda:0')\n",
      "\tdual residual = tensor(0.00090, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 12\n",
      "\tlog likelihood = 3162.85595703125\n",
      "\tprimal residual = tensor(0.01195, device='cuda:0')\n",
      "\tdual residual = tensor(0.00108, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 13\n",
      "\tlog likelihood = 3162.36279296875\n",
      "\tprimal residual = tensor(0.01228, device='cuda:0')\n",
      "\tdual residual = tensor(0.00059, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 14\n",
      "\tlog likelihood = 3162.412109375\n",
      "\tprimal residual = tensor(0.01195, device='cuda:0')\n",
      "\tdual residual = tensor(0.00080, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 15\n",
      "\tlog likelihood = 3161.3984375\n",
      "\tprimal residual = tensor(0.01233, device='cuda:0')\n",
      "\tdual residual = tensor(0.00025, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 16\n",
      "\tlog likelihood = 3161.040771484375\n",
      "\tprimal residual = tensor(0.01232, device='cuda:0')\n",
      "\tdual residual = tensor(0.00101, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 17\n",
      "\tlog likelihood = 3160.798095703125\n",
      "\tprimal residual = tensor(0.01248, device='cuda:0')\n",
      "\tdual residual = tensor(0.00062, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 18\n",
      "\tlog likelihood = 3162.42333984375\n",
      "\tprimal residual = tensor(0.01192, device='cuda:0')\n",
      "\tdual residual = tensor(0.00090, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 19\n",
      "\tlog likelihood = 3159.896728515625\n",
      "\tprimal residual = tensor(0.01161, device='cuda:0')\n",
      "\tdual residual = tensor(0.00104, device='cuda:0')\n",
      "\n",
      "[INFO] Epoch 20\n"
     ]
    }
   ],
   "source": [
    "args\n",
    "mu_tmp = learn_one_seq_penalty(args, x_input, y_input, penalty=10000)\n",
    "# m = args.num_samples\n",
    "# kappa = args.kappa\n",
    "# d = args.z_dim\n",
    "# pen_iter = 0\n",
    "# # penalty = penalty\n",
    "# early_stopping = False\n",
    "# stopping_count = 0 # for ADMM\n",
    "# # create matrix X and vector 1\n",
    "# T = args.num_time\n",
    "# mu_repeat = np.repeat(mu_tmp.cpu().numpy(), np.repeat(m, T), axis=0)\n",
    "# mu_repeat[0:10000,]\n",
    "# mu_repeat[10001:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b11b60c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104],\n",
      "        [ 0.00045, -0.00109, -0.00104]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(mu)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 8))\n",
    "# subplot 1: delta_mu\n",
    "axes[0].plot(delta_mu)\n",
    "axes[0].set_title(\"Δμ (row diff norm)\")\n",
    "\n",
    "# # subplot 2: log-likelihood\n",
    "# axes[0,1].plot(loglik_train_holder[1:])\n",
    "# axes[0,1].set_title(\"Training log-likelihood\")\n",
    "\n",
    "# subplot 3: mu difference holder\n",
    "axes[1].plot(mu_diff_holder[1:])\n",
    "axes[1].set_title(\"μ diff holder\")\n",
    "\n",
    "# # subplot 4: CV holder\n",
    "# axes[1,1].plot(CV_holder)\n",
    "# axes[1,1].set_title(\"CV holder\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "30fa3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "\n",
    "\n",
    "def learn_one_seq_penalty(args,x_input, y_input, pen_iter, seq_iter, half):\n",
    "  torch.manual_seed(1)\n",
    "  \n",
    "  m = args.num_samples\n",
    "  kappa = args.kappa\n",
    "  d = args.z_dim\n",
    "  penalty = args.penalties[pen_iter]\n",
    "  early_stopping = False\n",
    "  stopping_count = 0 # for ADMM\n",
    "\n",
    "  # if half:\n",
    "  #   T = int(args.num_time/2)\n",
    "  #   true_CP = args.true_CP_half\n",
    "  #   label = 'half'\n",
    "  # else:\n",
    "  #   T = args.num_time\n",
    "  #   true_CP = args.true_CP_full\n",
    "  #   label = 'full'\n",
    "\n",
    "  # create matrix X and vector 1\n",
    "  ones_col = torch.ones(T, 1).to(device)\n",
    "  X = torch.zeros(T, T-1).to(device)\n",
    "  i, j = torch.tril_indices(T, T-1, offset=-1)\n",
    "  X[i, j] = 1 # Group Fused Lasso\n",
    "\n",
    "  old_loglik = -float('inf')\n",
    "  loglik_train_holder = []\n",
    "  loglik_test_holder = []\n",
    "  mu_diff_holder = []\n",
    "  decoder_loss_holder = []\n",
    "  CV_holder = []\n",
    "\n",
    "\n",
    "  # use Coefficient of Variation (CV) when half=True\n",
    "  # save result based on Coefficient of Variation (CV)\n",
    "  if not half:\n",
    "    best_mu = torch.zeros(T,d)\n",
    "    best_loglik = torch.zeros(1)\n",
    "    best_CV = -float('inf') # Coefficient of Variation\n",
    "    best_CV_iter = 0\n",
    "\n",
    "  \n",
    "  # initialize mu, nu, w, with dim T by d of zeros\n",
    "  mu = torch.zeros(T, d).to(device)\n",
    "  nu = torch.zeros(T, d).to(device)\n",
    "  w = torch.zeros(T, d).to(device)\n",
    "  \n",
    "  mu_old = mu.detach().clone()\n",
    "  nu_old = nu.detach().clone()\n",
    "\n",
    "\n",
    "  # creat repeated version of ground truth, from (T by n by n) to (Tm by n by n)\n",
    "  # repeat m for T times, giving [m, m, ..., m]\n",
    "  # for each t in T (axis=0), repeat num_samples times, giving (Tm by n by n)\n",
    "  # y_train_repeat = np.repeat(y_train.numpy(), np.repeat(m, T), axis=0) \n",
    "  # y_train_repeat = torch.from_numpy(y_train_repeat).to(device)\n",
    "\n",
    "  # if half:\n",
    "  #   # these objects exist if half = True\n",
    "  #   y_test_repeat = np.repeat(y_test.numpy(), np.repeat(m, T), axis=0) \n",
    "  #   y_test_repeat = torch.from_numpy(y_test_repeat).to(device)\n",
    "\n",
    "\n",
    "  model = CPD(args, half).to(device)\n",
    "  model.apply(init_weights)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.decoder_lr)\n",
    "  criterion = nn.MSELoss(reduction='sum') # loglik sum over t, but expectation over m, so later divided by m\n",
    "  \n",
    "  for learn_iter in range(args.epoch):\n",
    "\n",
    "    ####################\n",
    "    # GENERATE SAMPLES #\n",
    "    ####################\n",
    "    # create repeated version of mu, from (T by d) to (Tm by d)\n",
    "    mu_repeat = np.repeat(mu.cpu().numpy(), np.repeat(m, T), axis=0)\n",
    "    mu_repeat = torch.from_numpy(mu_repeat).to(device) # Tm by d\n",
    "    init_z = torch.randn(T*m, d).to(device) # Tm by d, starts from N(0,1)\n",
    "    sampled_z_all = model.infer_z(x_input, init_z, y_input, mu_repeat, args) # Tm by d\n",
    "\n",
    "    ################\n",
    "    # UPDATE PRIOR # \n",
    "    ################\n",
    "    expected_z = sampled_z_all.clone().reshape(T,m,d) # T by m by d\n",
    "    expected_z = expected_z.mean(dim=1) # T by d\n",
    "    mu = ( expected_z + kappa * (nu-w) ) / ( 1.0 + kappa )\n",
    "    mu = mu.detach().clone()\n",
    "\n",
    "    ##################\n",
    "    # UPDATE DECODER #\n",
    "    ##################\n",
    "    inner_loss = float('inf')\n",
    "    for decoder_iter in range(args.decoder_iteration):\n",
    "      optimizer.zero_grad()\n",
    "      pi, mean, sigma, covs  = model(x_input, sampled_z_all) # Tm by n by n \n",
    "      loss = mixture_of_gaussians_loss(y_input, pi, mean, covs) / m\n",
    "      loss.backward()\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "      optimizer.step()\n",
    "\n",
    "    #########################\n",
    "    # UPDATE BETA AND GAMMA #\n",
    "    #########################\n",
    "\n",
    "    gamma = nu[0, :].unsqueeze(0) # row vector\n",
    "    beta = torch.diff(nu, dim=0)\n",
    "\n",
    "    for nu_iter in range(args.nu_iteration):\n",
    "      # update beta once (t range from 1 to 99, NOT 1 to 100)\n",
    "      for t in range(T-1): \n",
    "        beta_without_t = beta.detach().clone()\n",
    "        X_without_t = X.detach().clone()\n",
    "        beta_without_t[t,:] = torch.zeros(d) # make this row zeros\n",
    "        X_without_t[:,t] = torch.zeros(T) # make this column zeros\n",
    "        bt = kappa * torch.mm( X[:,t].unsqueeze(0), mu + w - torch.mm(ones_col, gamma) - torch.mm(X_without_t, beta_without_t) )\n",
    "        bt_norm = torch.norm(bt, p=2)\n",
    "\n",
    "        # UPDATE: soft-thresholding\n",
    "        if bt_norm < penalty:\n",
    "          beta[t,:] = torch.zeros(d)\n",
    "        else:\n",
    "          beta[t,:] = 1 / (kappa * torch.norm(X[:,t], p=2)**2) * (1 - penalty/bt_norm) * bt\n",
    "        beta = beta.detach().clone()\n",
    "\n",
    "      # update gamma\n",
    "      gamma = torch.mean(mu + w - torch.mm(X, beta), dim=0).unsqueeze(0).detach().clone()\n",
    "\n",
    "    # recollect nu\n",
    "    nu = torch.mm(ones_col, gamma) + torch.mm(X, beta)\n",
    "    nu = nu.detach().clone()\n",
    "\n",
    "    ############\n",
    "    # UPDATE W # \n",
    "    ############\n",
    "\n",
    "    w = mu - nu + w\n",
    "    w = w.detach().clone()\n",
    "\n",
    "    ############\n",
    "    # RESIDUAL # \n",
    "    ############\n",
    "\n",
    "    primal_residual = torch.sqrt(torch.mean(torch.square(mu - nu)))\n",
    "    dual_residual = torch.sqrt(torch.mean(torch.square(nu - nu_old)))\n",
    "\n",
    "    if primal_residual > 10.0 * dual_residual:\n",
    "      kappa *= 2.0\n",
    "      w *= 0.5\n",
    "      print('\\n[INFO] kappa increased to', kappa)\n",
    "    elif dual_residual > 10.0 * primal_residual:\n",
    "      kappa *= 0.5\n",
    "      w *= 2.0\n",
    "      print('\\n[INFO] kappa decreased to', kappa)\n",
    "\n",
    "    \n",
    "    # calculate log_likelihood\n",
    "    with torch.no_grad():\n",
    "      # if half=False, adj_test_repeat does not exist\n",
    "      # if half:\n",
    "      #   loglik_train = model.cal_loglik(mu, y_train_repeat) # USE TRAIN\n",
    "      #   loglik_train_holder.append(loglik_train.detach().cpu().numpy().item())\n",
    "      #   loglik = model.cal_loglik(mu, adj_test_repeat) # USE TEST\n",
    "      #   loglik_test_holder.append(loglik.detach().cpu().numpy().item())\n",
    "      # else:\n",
    "      #   loglik = model.cal_loglik(mu, adj_train_repeat) # USE TRAIN\n",
    "      #   loglik_train_holder.append(loglik.detach().cpu().numpy().item())\n",
    "      \n",
    "      # criteria 1\n",
    "      # loglik_relative_diff = torch.abs((loglik - old_loglik) / old_loglik)\n",
    "      # old_loglik = loglik.detach().clone()\n",
    "      \n",
    "\n",
    "      # criteria 2\n",
    "      mu_relative_diff = torch.norm(mu-mu_old, p='fro')\n",
    "      mu_diff_holder.append(mu_relative_diff.detach().cpu().numpy().item())\n",
    "      \n",
    "      mu_old = mu.detach().clone()\n",
    "      nu_old = nu.detach().clone()\n",
    "\n",
    "    #####################\n",
    "    # STOPPING CRITERIA #\n",
    "    #####################\n",
    " \n",
    "    # if loglik_relative_diff < args.loglik_thr:\n",
    "    #   stopping_count += 1\n",
    "    # else:\n",
    "    #   stopping_count = 0\n",
    "\n",
    "    # if stopping_count >= args.iter_thr:\n",
    "    #   print('\\n[INFO] early stopping')\n",
    "    #   early_stopping = True\n",
    "\n",
    "\n",
    "\n",
    "    ##############\n",
    "    # PRINT INFO #\n",
    "    ##############\n",
    "\n",
    "    if (learn_iter+1) % 10 == 0:\n",
    "\n",
    "      with torch.no_grad():\n",
    "        # second row - first row\n",
    "        delta_mu = torch.norm(torch.diff(mu, dim=0), p=2, dim=1)\n",
    "        delta_mu = delta_mu.cpu().detach().numpy() # numpy for plot\n",
    "\n",
    "        # plt.plot(delta_mu); plt.xticks(true_CP)\n",
    "        # plt.savefig( output_dir + '/{}_delta_mu_seq{}pen{}_learn{}'.format(label,seq_iter,pen_iter,learn_iter+1) + '.png' ) \n",
    "        # plt.close()\n",
    "\n",
    "        # if half: \n",
    "        #   plt.plot(loglik_train_holder[1:], label=\"train\")\n",
    "        #   plt.plot(loglik_test_holder[1:], label=\"test\")\n",
    "        #   plt.legend(loc=\"lower right\")\n",
    "        #   plt.savefig( output_dir + '/{}_loglik_seq{}pen{}'.format(label,seq_iter,pen_iter) + '.png' ) \n",
    "        #   plt.close()\n",
    "        # else:\n",
    "        #   plt.plot(loglik_train_holder[1:])\n",
    "        #   plt.savefig( output_dir + '/{}_loglik_seq{}pen{}'.format(label,seq_iter,pen_iter) + '.png' ) \n",
    "        #   plt.close()\n",
    "\n",
    "        # plt.plot(mu_diff_holder[1:])\n",
    "        # plt.savefig( output_dir + '/{}_mu_diff_seq{}pen{}'.format(label,seq_iter,pen_iter) + '.png' ) \n",
    "        # plt.close()\n",
    "\n",
    "        # if not half:\n",
    "        #   plt.plot(CV_holder)\n",
    "        #   plt.savefig( output_dir + '/{}_CV_seq{}pen{}'.format(label,seq_iter,pen_iter) + '.png' ) \n",
    "        #   plt.close()\n",
    "\n",
    "        # print('\\nlearning iter (seq={}, [penalty={}], data={}) ='.format(seq_iter,penalty,label), learn_iter+1, 'of', args.epoch)\n",
    "        # print('\\tlog likelihood =', loglik)\n",
    "        print('\\tprimal residual =', primal_residual)\n",
    "        print('\\tdual residual =', dual_residual)\n",
    "        # print('\\t\\tlog likelihood relative difference =', loglik_relative_diff)\n",
    "        print('\\t\\tmu relative difference =', mu_relative_diff)\n",
    "      \n",
    "    ###############\n",
    "    # SAVE RESULT #\n",
    "    ###############\n",
    "    # at the last iteration or early_stopping\n",
    "    # if (learn_iter+1) == args.epoch or early_stopping:\n",
    "    #   print('\\nFINAL learning iter (seq num = {}, [penalty={}]) ='.format(seq_iter,penalty), learn_iter+1, 'of', args.epoch)\n",
    "    #   # print('FINAL log likelihood =', loglik)\n",
    "    #   # print('FINAL log likelihood relative difference =', loglik_relative_diff)\n",
    "    #   print('FINAL mu relative difference =', mu_relative_diff)\n",
    "\n",
    "    #   with torch.no_grad():\n",
    "    #     if half:\n",
    "    #       # USE THE LAST MU\n",
    "    #       delta_mu = torch.norm(torch.diff(mu, dim=0), p=2, dim=1)\n",
    "    #       result = evaluation(delta_mu, args, loglik, pen_iter, seq_iter, output_dir, half)\n",
    "    #       return torch.tensor(result[0:5]), loglik, mu\n",
    "    #     else:\n",
    "    #       # USE THE BEST MU\n",
    "    #       print('[INFO] best_CV_iter =', best_CV_iter)\n",
    "    #       delta_mu = torch.norm(torch.diff(best_mu, dim=0), p=2, dim=1)\n",
    "    #       result = evaluation(delta_mu, args, best_loglik, pen_iter, seq_iter, output_dir, half)\n",
    "    #       return torch.tensor(result[0:5]), best_loglik, best_mu, w_left, w_right\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7adcc135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] kappa increased to 20.0\n",
      "\n",
      "[INFO] kappa increased to 40.0\n",
      "\n",
      "[INFO] kappa increased to 80.0\n",
      "\n",
      "[INFO] kappa increased to 160.0\n",
      "\n",
      "[INFO] kappa increased to 320.0\n",
      "\n",
      "[INFO] kappa increased to 640.0\n",
      "\n",
      "[INFO] kappa increased to 1280.0\n",
      "\n",
      "[INFO] kappa increased to 2560.0\n",
      "\n",
      "[INFO] kappa increased to 5120.0\n",
      "\n",
      "[INFO] kappa increased to 10240.0\n",
      "\tprimal residual = tensor(3.08778e-05, device='cuda:0')\n",
      "\tdual residual = tensor(9.58394e-07, device='cuda:0')\n",
      "\t\tmu relative difference = tensor(0.00196, device='cuda:0')\n",
      "\n",
      "[INFO] kappa increased to 20480.0\n",
      "\n",
      "[INFO] kappa increased to 40960.0\n",
      "\n",
      "[INFO] kappa increased to 81920.0\n",
      "\n",
      "[INFO] kappa increased to 163840.0\n",
      "\n",
      "[INFO] kappa increased to 327680.0\n",
      "\n",
      "[INFO] kappa increased to 655360.0\n",
      "\n",
      "[INFO] kappa increased to 1310720.0\n",
      "\n",
      "[INFO] kappa increased to 2621440.0\n",
      "\n",
      "[INFO] kappa increased to 5242880.0\n",
      "\n",
      "[INFO] kappa increased to 10485760.0\n",
      "\tprimal residual = tensor(3.10123e-08, device='cuda:0')\n",
      "\tdual residual = tensor(1.67730e-09, device='cuda:0')\n",
      "\t\tmu relative difference = tensor(1.90991e-06, device='cuda:0')\n",
      "\n",
      "[INFO] kappa increased to 20971520.0\n",
      "\n",
      "[INFO] kappa increased to 41943040.0\n",
      "\n",
      "[INFO] kappa increased to 83886080.0\n",
      "\n",
      "[INFO] kappa increased to 167772160.0\n",
      "\n",
      "[INFO] kappa increased to 335544320.0\n",
      "\n",
      "[INFO] kappa increased to 671088640.0\n",
      "\n",
      "[INFO] kappa increased to 1342177280.0\n",
      "\n",
      "[INFO] kappa increased to 2684354560.0\n",
      "\n",
      "[INFO] kappa increased to 5368709120.0\n",
      "\tprimal residual = tensor(5.82274e-11, device='cuda:0')\n",
      "\tdual residual = tensor(1.78224e-12, device='cuda:0')\n",
      "\t\tmu relative difference = tensor(3.89947e-09, device='cuda:0')\n",
      "\n",
      "[INFO] kappa increased to 10737418240.0\n",
      "\n",
      "[INFO] kappa increased to 21474836480.0\n",
      "\n",
      "[INFO] kappa increased to 42949672960.0\n",
      "\n",
      "[INFO] kappa increased to 85899345920.0\n",
      "\n",
      "[INFO] kappa increased to 171798691840.0\n",
      "\n",
      "[INFO] kappa increased to 343597383680.0\n",
      "\tprimal residual = tensor(0., device='cuda:0')\n",
      "\tdual residual = tensor(0., device='cuda:0')\n",
      "\t\tmu relative difference = tensor(0., device='cuda:0')\n",
      "\n",
      "[INFO] kappa increased to 687194767360.0\n",
      "\tprimal residual = tensor(1.48520e-13, device='cuda:0')\n",
      "\tdual residual = tensor(0., device='cuda:0')\n",
      "\t\tmu relative difference = tensor(3.63798e-12, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "learn_one_seq_penalty(args, x_input, y_input, pen_iter=0, seq_iter=1, half=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CPD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
